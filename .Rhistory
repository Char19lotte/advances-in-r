?normalize
scale
getAnywhere(scale.default())
methods(scale)
getAnywhere(scale.default())
?scale
normalize <- function(x, na.rm = FALSE) {
y <-  (x - mean(x, na.rm = na.rm))/sd(x, na.rm = na.rm)
y
}
vec <- c(1, 5, 10, NA)
normal(vec, na.rm = TRUE)
normalize(vec, na.rm = TRUE)
normalize(vec, na.rm = TRUE) %>% summary
normalize(vec, na.rm = TRUE) %>% sd
normalize(vec, na.rm = TRUE) %>% sd(na.rm = TRUE)
vec <- c(1, 5, 10, NA)
normalize(vec, na.rm = TRUE) %>% summary
normalize(vec, na.rm = TRUE) %>% sd(na.rm = TRUE)
j <- function() {
if (!exists("a")) {
a <- 1
} else {
a <- a + 1
}
print(a)
}
j()
j()
j()
j()
j <- function() {
if (!exists("a")) {
a <- 1
} else {
a <- a + 1
}
print(a)
}
a <- 3
j()
j()
j()
j()
j()
rm(a, j)
x <- 10
f <- function() x
f()
f <- function() x
f()
x <- 15
f()
x <- 20
f()
x <- 10
f <- function() x
f()
x <- 15
f()
x <- 20
f()
f <- function() x + 1
codetools::findGlobals(f)
f <- function(x) sqrt(x)
codetools::findGlobals(f)
f <- function(x) sqrt(x)
codetools::findGlobals(f)
??codetools
x <- list(1:3, 4:9, 10:12)
sapply(x, "[", 2)
args <- list(1:10, na.rm = TRUE)
args
do.call(mean, args)
g <- function(a = 1, b = a * 2) {
c(a, b)
}
g()
g(10)
g(10, 30)
# you can determine if an argument was supplied or not with the missing() function
i <- function(a, b) {
c(missing(a), missing(b))
}
i()
i(a = 1)
# R function arguments are lazy---they're only evaluated if they're actually used:
f <- function(x) {
10
}
f(stop("This is an error!"))
f <- function(x) {
force(x)
10
}
f(stop("This is an error!"))
`%+%` <- function(a, b) paste(a, b, sep = "")
"new" %+% " string"
function_that_might_return_null(my_favorite_number, 3)
`%||%` <- function(a, b) if (!is.null(a)) a else b
function_that_might_return_null() %||% default value
function_that_might_return_null() %||% 3
?xor
NULL %||% 3
NULL && 4
# the last expression evaluated in a function becomes the return value_
f <- function(x) {
if (x < 10) {
0
} else {
10
}
}
f()
f(1)
?return
?returnValue
getwd()
in_dir <- function(dir, code) {
old <- setwd(dir)
on.exit(setwd(old))
force(code)
}
in_dir("~", getwd())
getwd()
getwd()
in_dir("~", getwd())
divideFun <- function(x, y) {
res <- x / y
on.exit(plot(x), add = TRUE)
return(res)
}
divideFun(1, 2)
divideFun <- function(x, y) {
res <- x / y
on.exit(message("Everything is fine."))
return(res)
}
divideFun(4)
divideFun(1, 2)
divideFun(1, 2)
divideFun <- function(x, y) {
res <- x / y
return(res)
on.exit(message("Everything is fine."))
}
divideFun(1, 2)
divideFun <- function(x, y) {
res <- x / y
res
on.exit(message("Everything is fine."))
}
divideFun(1, 2)
divideFun(4)
divideFun <- function(x, y) {
res <- x / y
res
on.exit(message("Everything is fine."), add = TRUE)
}
divideFun(4)
?on.exit
c <- 10
c(c = c)
source("00-course-setup.r")
wd <- getwd()
string1 <- "This is a string"
string2 <- 'If I want to include a "quote" inside a string, I use single quotes'
double_quote <- "\"" # or '"'
single_quote <- '\'' # or "'"
# many special characters around, see
?"'"
double_quote
writeLines(double_quote) # shows raw contents of the string
x <- "\u00b5"
x
writeLines(x)
x <- c("apple", "banana", "pear")
str_view(x, "an")
str_view_all(x, "an")
str_view_all(x, "a")
## string matching ----------
# example
raw.data <- "555-1239Moe Szyslak(636) 555-0113Burns, C. Montgomery555-6542Rev. Timothy Lovejoy555 8904Ned Flanders636-555-3226Simpson, Homer5553642Dr. Julius Hibbert"
name <- unlist(str_extract_all(raw.data, "[[:alpha:]., ]{2,}"))
name
phone <- unlist(str_extract_all(raw.data, "\\(?(\\d{3})?\\)?(-| )?\\d{3}(-| )?\\d{4}"))
phone
data.frame(name = name, phone = phone)
# running example
example.obj <- "1. A small sentence. - 2. Another tiny sentence."
# self match
str_extract(example.obj, "small")
str_extract(example.obj, "banana")
# multiple matches
(out <- str_extract_all(c("text", "manipulation", "basics"), "a"))
# case sensitivity
(out <- str_extract_all(c("text", "manipulation", "basics"), "a"))
str_extract(example.obj, "small")
str_extract(example.obj, "SMALL")
str_extract(example.obj, ignore.case("SMALL"))
str_extract(example.obj, regex("SMALL", ignore_case =TRUE))
?str_extract
str_extract(example.obj, "mall sent")
# match the beginning of a string
str_extract(example.obj, "^1")
str_extract(example.obj, "^2")
# match the end of a string
str_extract(example.obj, "sentence$")
str_extract(example.obj, "sentence.$")
# pipe operator
unlist(str_extract_all(example.obj, "tiny|sentence"))
# wildcard
str_extract(example.obj, "sm.ll")
# character class
str_extract(example.obj, "sm[abc]ll")
# character class: range
str_extract(example.obj, "sm[a-p]ll")
# character class: additional characters
unlist(str_extract_all(example.obj, "[uvw. ]"))
# pre-defined character classes
unlist(str_extract_all(example.obj, "[[:punct:]]"))
unlist(str_extract_all(example.obj, "[:punct:]"))
unlist(str_extract_all(example.obj, "[[:punct:]ABC]"))
unlist(str_extract_all(example.obj, "[^[:alnum:]]"))
?regex
# additional shortcuts
?base::regex
unlist(str_extract_all(example.obj, "\\w+"))
unlist(str_extract_all(example.obj, "e\\b"))
unlist(str_extract_all(example.obj, "e\\B"))
str_extract(example.obj, "s[[:alpha:]][[:alpha:]][[:alpha:]]l")
str_extract(example.obj, "s[[:alpha:]]{3}l")
str_extract(example.obj, "A.+sentence")
# greedy quantification
str_extract(example.obj, "A.+sentence")
str_extract(example.obj, "A.+?sentence")
unlist(str_extract_all(example.obj, "(.en){1,5}"))
unlist(str_extract_all(example.obj, ".en{1,5}"))
unlist(str_extract_all(example.obj, "\\."))
unlist(str_extract_all(example.obj, fixed(".")))
# meta characters in character classes
unlist(str_extract_all(example.obj, "[1-2]"))
unlist(str_extract_all(example.obj, "[12-]"))
# backreferencing
str_extract(example.obj, "([[:alpha:]]).+?\\1")
str_extract(example.obj, "(\\b[a-z]+\\b).+?\\1")
# grouped matches
str_extract_all(example.obj, "([^ ]+) (sentence)")
str_match_all(example.obj, "([^ ]+) (sentence)")
# assertions
unlist(str_extract_all(example.obj, perl("(?<=2. ).+")))
browseURL("http://stackoverflow.com/questions/201323/using-a-regular-expression-to-validate-an-email-address/201378#201378") # think again
str_extract_all(example.obj, "([^ ]+) (sentence)")
str_match_all(example.obj, "([^ ]+) (sentence)")
example.obj
?regex
?perl
unlist(str_extract_all(example.obj, regex("(?<=2. ).+", perl = TRUE)))
unlist(str_extract_all(example.obj, regex("(?<=2. ).+")))
unlist(str_extract_all(example.obj, regex("(?<=2. ).+")))
unlist(str_extract_all(example.obj, "(?<=2. ).+"))
unlist(str_extract_all(example.obj, ".+(?=2)"))
unlist(str_extract_all(example.obj, "(?<!tiny ).+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "(?<!tiny).+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "(?<!tiny).+?")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "(?<!Another )tiny.+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "(?<!Another ).+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "(?<!Bla )tiny.+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, "(?<!Another )tiny.+")) # negative lookbehind: (?<!...)
unlist(str_extract_all(example.obj, ".+(?!2)")) # negative lookahead (?!...)
unlist(str_extract_all(example.obj, ".+(?!2)$")) # negative lookahead (?!...)
unlist(str_extract_all(example.obj, ".+(?!2)?")) # negative lookahead (?!...)
unlist(str_extract_all(example.obj, ".+?(?!2)")) # negative lookahead (?!...)
unlist(str_extract_all(example.obj, ".+(?!2)")) # negative lookahead (?!...)
unlist(str_extract_all(example.obj, ".+(?!3)")) # negative lookahead (?!...)
unlist(str_extract_all(example.obj, "sentence.+(?!3)")) # negative lookahead (?!...)
unlist(str_extract_all(example.obj, "sentence.+(?!Another)")) # negative lookahead (?!...)
unlist(str_extract_all(example.obj, "sentence.+(?!Another)?")) # negative lookahead (?!...)
unlist(str_extract_all(example.obj, "sentence.+?(?!Another)")) # negative lookahead (?!...)
unlist(str_extract(example.obj, "sentence.+?(?!Another)")) # negative lookahead (?!...)
unlist(str_extract_all(example.obj, "sentence.+?(?!Another)")) # negative lookahead (?!...)
unlist(str_extract_all(example.obj, "sentence.+?(?!Bla)")) # negative lookahead (?!...)
unlist(str_extract_all(example.obj, "sentence.+(?!Bla)")) # negative lookahead (?!...)
?stri_count_words
example.obj
stri_count_words(example.obj)
library(stringi)
stri_count_words(example.obj)
stri_stats_latex(example.obj)
stri_stats_general(example.obj)
stri_escape_unicode("\u00b5")
stri_unescape_unicode("\u00b5")
stri_rand_lipsum(3)
stri_rand_shuffle("hello")
stri_rand_strings(100, 10, pattern = "[mannheim]")
str_locate(example.obj, "tiny")
str_sub(example.obj, start = 35, end = 38)
# replacement
str_sub(example.obj, 35, 38) <- "huge"
str_replace(example.obj, pattern = "huge", replacement = "giant")
str_split(example.obj, "-") %>% unlist
str_split_fixed(example.obj, "[[:blank:]]", 5) %>% as.character()
(char.vec <- c("this", "and this", "and that"))
# detection
str_detect(char.vec, "this")
# keep strings matching a pattern
str_subset(char.vec, "this") # wrapper around x[str_detect(x, pattern)]
str_count(char.vec, "this")
str_count(char.vec, "\\w+")
str_length(char.vec)
str_dup(char.vec, 3)
# padding and trimming
length.char.vec <- str_length(char.vec)
char.vec <- str_pad(char.vec, width = max(length.char.vec), side = "both", pad = " ")
char.vec
str_trim(char.vec)
# joining
str_c("text", "manipulation", sep = " ")
str_c(char.vec, collapse = "\n") %>% cat
str_c("text", c("manipulation", "basics"), sep = " ")
agrep("Donald Trump", "Donald Drumpf", max.distance = list(all = 3))
agrep("Donald Trump", "Barack Obama", max.distance = list(all = 3))
email %>% str_replace("\\[at\\]", "@")
email <- "chunkylover53[at]aol[dot]com"
email %>% str_replace("\\[at\\]", "@")
email %>% str_replace("\\[at\\]", "@") %>% str_replace("\\[dot\\]", ".")
str_extract(email_new, "[:digit:]")
email_new <- email %>% str_replace("\\[at\\]", "@") %>% str_replace("\\[dot\\]", ".")
str_extract(email_new, "[:digit:]")
str_extract(email_new, "[:digit:]+")
str_extract_all("Phone 150$, PC 690$", "[0-9]+\\$") # example
str_extract_all("Just any sentence, I don't know. Today is a nice day.", "\\b[a-z]{1,4}\\b")
str_extract_all(c("log.txt", "example.html", "bla.txt2"), ".*?\\.txt$")
str_extract_all("log.txt, example.html, bla.txt2", ".*?\\.txt$")
str_extract_all(c("01/01/2000", "1/1/00", "01.01.2000"), "\\d{2}/\\d{2}/\\d{4}")
str_extract_all("<br>laufen</br>", "<title>Cameron wins election</title>", "<(.+?)>.+?</\\1>")
str_extract_all("<br>laufen</br>", "<title>Cameron wins election</title>", "<(.+?)>.+?</\\1>")
str_extract_all("<br>laufen</br>", "<title>Cameron wins election</title>", "<(.+?)>.+?</\\1>")
str_extract_all("<br>laufen</br>", "<(.+?)>.+?</\\1>")
str_extract_all("<br>laufen</br>", "<title>Cameron wins election</title>", "<(.+?)>.+?</\\1>")
str_extract("<br>laufen</br>", "<(.+?)>.+?</\\1>")
str_extract_all(c("<br>laufen</br>", "<title>Cameron wins election</title>"), "<(.+?)>.+?</\\1>")
browseURL("https://upload.wikimedia.org/wikipedia/en/b/b9/MagrittePipe.jpg")
browseURL("https://upload.wikimedia.org/wikipedia/en/b/b9/MagrittePipe.jpg")
dat <- babynames
dim(dat)
dat_filtered <- filter(dat, name == "Kim")
dat_grouped <- group_by(dat_filtered, year, sex)
dat_sum <- summarize(dat_grouped, total = sum(n))
qplot(year, total, color = sex, data = dat_sum, geom = "line") +
ggtitle('People named "Kim"')
dat <- summarize(group_by(filter(babynames, name == "Kim"), year, sex), total = sum(n))
qplot(year, total, color = sex, data = dat, geom = "line") +  ggtitle('People named "Kim"')
# magrittr style of piping code
babynames %>%
filter(name %>% equals("Kim")) %>%
group_by(year, sex) %>%
summarize(total = sum(n)) %>%
qplot(year, total, color = sex, data = ., geom = "line") %>%
add(ggtitle('People named "Kim"')) %>%
print
babynames %>%
subset(prop > mean(prop)) %$%
cor(year, prop)
data.frame(z = rnorm(100)) %$%
ts.plot(z)
foo <- foo %>% bar %>% baz # equal to...
foo %<>% bar %>% baz
babynames$prop %<>% sqrt
# functions in right-hand sides ("lambda expressions") --------
babynames %>%
{
n <- sample(1:10, size = 1)
H <- head(., n)
T <- tail(., n)
rbind(H, T)
} %>%
dim
library(pryr)
??pryr
?object_size
?switch
has_name <- function(x) {
nms <- names(x)
if (is.null(nms)) {
rep(FALSE, length(x))
} else {
!is.na(nms) & nms != ""
}
}
has_name(c(1, 2, 3))
has_name(NULL)
has_name(mtcars)
?stopifnot
wt_mean <- function(x, w, na.rm = FALSE) {
stopifnot(is.logical(na.rm), length(na.rm) == 1)
stopifnot(length(x) == length(w))
if (na.rm) {
miss <- is.na(x) | is.na(w)
x <- x[!miss]
w <- w[!miss]
}
sum(w * x) / sum(x)
}
wt_mean(1:6, 6:1, na.rm = "foo")
show_missings <- function(df) {
n <- sum(is.na(df))
cat("Missing values: ", n, "\n", sep = "")
invisible(df)
}
show_missings(mtcars)
source("00-course-setup.r")
wd <- getwd()
foo_df <- as.data.frame(matrix(ncol = 6))
names(foo_df) <- c("hIgHlo", "REPEAT VALUE", "REPEAT VALUE", "% successful (2009)",  "abc@!*", "")
foo_df
janitor::clean_names(foo_df)
make.names(names(foo_df)) # base R solution - not very convincing
# convert multiple values to NA
convert_to_NA(letters[1:5], c("b", "d"))
convert_to_NA(letters[1:5], c("b", "d"))
sample(c(1:5, 99), 10)
?sample
sample(c(1:5, 99), 10, replace = TRUE)
convert_to_NA(sample(c(1:5, 99), 20, replace = TRUE), 99)
convert_to_NA(sample(c(1:5, 99), 20, replace = TRUE), 99)
# clean frequency tables
head(mtcars)
table(mtcars$cyl)
janitor::tabyl(mtcars$cyl, show_na = TRUE, sort = TRUE)
janitor::tabyl(mtcars$cyl, show_na = TRUE, sort = TRUE) %>% add_totals_row()
# clean cross tabulations
table(mtcars$cyl, mtcars$gear)
mtcars %>% crosstab(cyl, gear)
mtcars %$% table(cyl, gear)
mtcars %>% crosstab(cyl, gear)
mtcars %>% janitor::crosstab(cyl, gear)
mtcars %>% janitor::crosstab(cyl, gear) %>% adorn_crosstab(denom = "row", show_totals = TRUE)
# use first valid value of multiple variables to get rid of NAs
set.seed(123)
x <- sample(c(1:10, rep(NA, 5)))
y <- sample(c(1:10, rep(NA, 5)))
z <- sample(c(1:10, rep(NA, 5)))
foo_df <- data.frame(x, y, z)
foo_df
foo_df %$% use_first_valid_of(x, y, z))
foo_df %$% use_first_valid_of(x, y, z)
foo_df %$% ifelse(!is.na(x), x, ifelse(!is.na(y), y, ifelse(!is.na(z), z, NA)))
foo_df
model_out <- lm(mpg ~ wt, mtcars) # linear relationship between miles/gallon and weight (in 1000 lbs)
model_out
summary(model_out)
str(model_out)
coef(summary(model_out)) # matrix of coefficients with variable terms in row names
broom::tidy(model_out)
broom::augment(model_out) %>% head
broom::glance(model_out)
data(Orange)
head(Orange)
cor(Orange$age, Orange$circumference)
data(Orange)
head(Orange)
Orange
cor(Orange$age, Orange$circumference)
ggplot(Orange, aes(age, circumference, color = Tree)) + geom_line()
Orange %>% group_by(Tree) %>% summarize(correlation = cor(age, circumference))
cor.test(Orange$age, Orange$circumference)
Orange %>% group_by(Tree) %>% do(tidy(cor.test(.$age, .$circumference)))
# also works for regressions
Orange %>% group_by(Tree) %>% do(tidy(lm(age ~ circumference, data=.)))
regressions <- mtcars %>% group_by(cyl) %>%
do(fit = lm(wt ~ mpg + qsec + gear, .))
regressions
regressions %>% tidy(fit)
regressions %>% augment(fit)
regressions %>% glance(fit)
browseURL("http://r4ds.had.co.nz/tidy-data.html") # much about reshaping and stuff; really necessary?
table4a
data(table4a)
data(table4)
library(tidyverse)
install.packages("tidyverse")
library(tidyverse)
table1
tabl2
table2
library(tidyr)
?table1
source("00-course-setup.r")
wd <- getwd()
table1
table1
table %>% mutate(rate = cases / population * 10000)
table %>% dplyr::mutate(rate = cases / population * 10000)
table1 %>% mutate(rate = cases / population * 10000)
table1 %>% count(year, wt = cases)
View(table1)
table2
table4a
browseURL("https://twitter.com/FrederikAust/status/789101346595151872/photo/1")
table4a
table4a %>% gather(`1999`, `2000`, key = "year", value = "cases")
table4b
table4a
table4b
table4b %>% gather(`1999`, `2000`, key = "year", value = "population")
(tidy4a <- table4a %>% gather(`1999`, `2000`, key = "year", value = "cases"))
(tidy4b <- table4b %>% gather(`1999`, `2000`, key = "year", value = "population"))
left_join(tidy4a, tidy4b)
?left_join
table2
(tidy2 <- table2 %>% spread(key = type, value = count))
